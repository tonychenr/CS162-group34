           +--------------------+
            |        CS 162      |
            | PROJECT 1: THREADS |
            |   DESIGN DOCUMENT  |
            +--------------------+
                   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Prudhvi Lokireddy <prudhvil@berkeley.edu>
Tony Chen <sirtony@berkeley.edu>
Khalid Shakur <khalidshakur@berkeley.edu>
Nick Wissman <nickwissman@berkeley.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

                 ALARM CLOCK
                 ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

static struct intr_lock; \\ This lock must be acquired in timer_sleep() before interrupts can be disabled or enabled.

A variable is added to the thread struct to keep track of the time left to sleep.
struct thread
  {
    /* Time for the thread to sleep*/
    int sleep_ticks;
  };

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.
The thread calling timer_sleep() would have it’s sleep_ticks set to the time specified in the arguments of timer_sleep().
Then, thread_block() is called in timer_sleep().
The timer interrupt handler would decrement the sleep_ticks of each thread during each tick.
If sleep_ticks becomes 0 after decrementing, the thread is unblocked with thread_unblock().

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?
The perform the minimum number of tasks required to perform the sleep routine.
The ticks must be decremented with each tick of the timer, so that must be done in the timer interrupt.
The thread wouldn’t know when to wake up unless some mechanism periodically checks to see if the sleep time is over.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?
There could be a race condition where both threads disable interrupts.
Then, one thread finishes and enables interrupts before the other one has called thread_block().
To prevent this, we defined a global lock on interrupt setting.
This requires each timer_sleep() call to finish before another one starts.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?
Interrupts are disabled at the beginning timer_sleep(), so a timer interrupt won’t occur during a call to timer_sleep().
Interrupts are reenabled at the end of the function.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?
We considered storing the sleep_ticks in a hashtable.
This would be a waste of memory and the size may need to be increased to accommodate more threads or decreased to not waste memory.
We could also place an if statement inside the intr_disable() and intr_enable() functions, instead of having a lock for interrupt disabling/enabling.
We chose this design, because it is easier to see the purpose and is simpler for others reading the code to understand.
The disadvantage would be that the system may panic when someone accidentally disables or enables interrupts at the wrong times, due to race conditions.

             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

The following variables are added to the thread stub to keep track of donators and donations to another thread.
struct thread {
  /* The thread’s original priority, used to reset the priority after being donated to */
int original_priority;
/* The list of donator_elems that are donating priority to this thread */
list donators;
/* A pointer to the thread that owns the lock this thread is waiting on; null if it’s not waiting for a lock */
thread* wait_holder;
/* A pointer to the corresponding donator_elem in the child’s list */
struct donator_elem *donator_elem;
}

This struct holds information about a donator which is donating to this thread.
struct donator_elem {
  /* A pointer to a lock being requested by another thread */
struct lock* parent_lock;
/* The list_elem that allows this struct to be placed in the list */
struct list_elem elem;
/* The highest priority of any thread requesting the parent_lock *
int priority;
}


>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)
- Original priority stores each thread's priority value before donations so that it may be restored once it releases all locks and donating becomes irrelevant
- Donator list is composed of donator_elem structs, where donator_elem structs store the necessary donation information to determine priority values based on 
  donation
- The wait holder keeps track of the thread to which this thread is donating.

Diagram:

Lock HolderB is the thread holding Lock1, Lock2, and Lock 3. B is waiting on a lock from A. 
Bdonator_elem is the donator linked-list for threads waiting on a lock from B, along with the associated lock for each donator.
Bwait_holder is the thread holding the lock upon which B is waiting.

        Bwait_holder(Lock HolderA (original_priority 1, priority 4))
                             |
                             |  
        Lock HolderB (original_priority 2, priority 4)]
                     /            |                  \
                    /             |                   \
Bdonator_elem[{Lock1,Donator(2)} {Lock2, Donator(3)} {Lock3, Donator(4)}]

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?
Semaphores, instead of using list_pop_front() to get the next thread to run, use list_max() and list_remove(). Because lock and condition variables are implemented using semaphores, this will work for all three. 

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

The thread calling lock_acquire() sets its child to the thread that currently owns the lock.
Then, it checks the child’s parent_list to see if there is already a parent_elem corresponding to the lock it is requesting.
If there is, it sets its own my_elem to that parent_elem, and sets the my_elem’s priority to the higher value of its own priority or the previous value of the entry’s priority.
It then reinserts it to keep the list sorted by priority.
If there is not, it creates a new parent_elem where parent_lock points to the lock it’s requesting and priority is its own priority.
It then sets the child thread’s priority to the greater value of its own priority and the child’s priority.
Then, it iterates through each child’s child, repeating the process until it gets to a thread with no child (and thus is not waiting on a lock).
This would be done through a for/while loop.
If a thread along the way already has a my_elem, then we set that my_elem’s priority to the current thread’s priority if it’s higher.
Otherwise, we know it doesn’t have a child and stop the loop.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.
First, the thread that releases the lock removes the parent_elem corresponding to that lock from the parents list.
Then, sets its priority to either original_priority (if parents list is empty) or the highest priority in parents list.
Then, the higher-priority thread is granted acquisition of the lock as it is the first element on the locks waiting list. 

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?
There is a potential race when the thread_set_priority() is called after thread_get_priority() when a thread holds multiple locks and has multiple threads trying to donate priority.
Both threads may think that the current priority is lower than it actually is, because another may have updated the priority.
We would fix this by acquiring a lock before getting the current priority and then setting the priority.


---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?
In choosing our design, we considered memory limitations and time complexity of the data structure algorithms.
In the end, we decided on storing all the information in the thread stub.
Alternatives we considered were storing the lists of donators in a hash table or tree in the kernel as a global struct.
Making this information global may be bad for modularity reasons, because it might clutter the list of global variables.
This implementation does not take up too much memory in the thread stub, and allows us to access children immediately instead of having to search through a tree or lookup in a hashtable.
Also, it is much simpler and more natural of a design than the other structures.
The algorithm to iterate through children arises naturally from our structure.

              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

static int load_avg; \\ this stores the variable load_avg as described in the spec; located in thread.c

Each thread needs to keep track of its own recent_cpu.
struct thread{
  int recent_cpu; \\ This variable holds the calculated value of recent_cpu
}

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
0      0   0   0   63  61  59   A  
4      4   0   0   62  61  59   A
8      8   0   0   61  61  59   B
12     8   4   0   61  60  59   A
16     12  4   0   60  60  59   B
20     12  8   0   60  59  59   A
24     16  8   0   59  59  59   C
28     16  8   4   59  59  58   B
32     16  12  4   59  58  58   A
36     20  12  4   58  58  58   C


>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?
We picked by the least recently executed thread when the priorities were identical.
This matches the behavior of our scheduler through the following idea.
We would list_push_back() to insert a thread back into the ready queue, so the most recently run thread (elem) gets inserted into the ready queue at the back of those with the same priority.
The method, list_max(), gets the maximum that appears earliest in the list.
A list_remove() would remove that thread from the queue to be run.
This is round robin for threads with the same priority.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?
Everything having to do with incrementing recent_cpu and calculating priority, recent_cpu, and load_avg happens inside the timer interrupt, because these operations must be run at specific times.
Adding and multiplication is not very CPU intensive.
Performance may be bad when there are many threads for which there are values to increment and recalculate.
The handling of popping threads from the ready queue and pushing threads into the ready queue is outside of the interrupt handler.
Popping the highest priority thread from the queue would take O(n) time where n is the number of threads in the ready queue.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?
Advantages:
Simple and easy to implement ready queue.
No sorting methods needed for ready queue.

Disadvantages:
Calculating priority and coefficients may slow down processes when there are many threads for which to calculated and increment values.

Having a sorted ready queue may make the process of selecting the next thread faster.
Inserting elements would take O(logn) time when the queue is sorted and popping the maximum would take O(1) time.
Right now it takes O(n) time to pop the maximum, but O(1) time to insert.
When priorities are recalculated, the queue may need to be resorted.
